{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Neural Networks\n",
    "\n",
    "![dogcat](img/dogcat.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Remember when we went from linear to logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![lin-log](img/linear_vs_logistic_regression.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use a linear combination of variables\n",
    "\n",
    "$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2, x_2 +\\ldots + \\beta_n x_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### And passed the sum product of those variables and coefficients through a sigmoid function.\n",
    "\n",
    "$$ P(y) = \\displaystyle \\frac{1}{1+e^{-(\\hat y)}}$$\n",
    "\n",
    "![sig](img/SigmoidFunction_701.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another way of writing this:\n",
    "\n",
    "![log-reg-der](img/log_reg_deriv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### If we change the orientation of the first part, we get a new diagram:\n",
    "![log-reg](img/log_reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### _Logistic Regression_ was our first introduction to a Neural Network(NN):\n",
    "![log-reg](img/log_reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A more general notation for a single layer NN:\n",
    "![fnn](img/First_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### New vocabulary!\n",
    "\n",
    "- input layer\n",
    "- weights\n",
    "- hidden layer\n",
    "- summation function\n",
    "- activation function\n",
    "- output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Input layer\n",
    "\n",
    "The input layer of a neural network is the list of variables we are using in our model.\n",
    "\n",
    "![input](img/log-reg-nn-ex-i.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weights\n",
    "\n",
    "In the case of logistic regression, the weights here are the coefficients we are adjusting to fit our model. In other Neural Networks, the weights are a combination of scalar transformations and matrix multiplication on any of the input variables.\n",
    "\n",
    "![act](img/log-reg-nn-ex-w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summation function\n",
    "\n",
    "![sum](img/log-reg-nn-ex-sum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation function\n",
    "\n",
    "In logistic regression we use a **sigmoid** activation function. Other options you might see are **linear**, **Tanh** and **ReLu**.<br>\n",
    "[Loss functions for neural networks](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
    "![act](img/log-reg-nn-ex-a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deeper networks = more hidden layers\n",
    "\n",
    "![dnn](img/Deeper_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why _hidden_ layers?\n",
    "\n",
    "They are hidden because we do not specify them. They could represent latent factors (as with matrix decomposition), or a combination of the existing variables into new features.\n",
    "\n",
    "![ic-nn](img/Ice_cream_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But why _neural_ ?\n",
    "\n",
    "![neuron-bio](img/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspiration from Actual Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"The signaling process is partly electrical and partly chemical. Neurons are electrically excitable, due to maintenance of voltage gradients across their membranes. If the voltage changes by a large enough amount over a short interval, the neuron generates an all-or-nothing electrochemical pulse called an action potential. This potential travels rapidly along the axon, and activates synaptic connections as it reaches them. Synaptic signals may be excitatory or inhibitory, increasing or reducing the net voltage that reaches the soma.\" (https://en.wikipedia.org/wiki/Neuron)\n",
    "\n",
    "Another important idea from neurology is the \"all-or-none\" principle: <br/> (https://en.wikipedia.org/wiki/Neuron#All-or-none_principle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The biology comparison \n",
    "\n",
    "Neural networks draw their inspiration from the biology of our own brains, which are of course also accurately described as 'neural networks'. A human brain contains around $10^{11}$ neurons, connected very densely.\n",
    "\n",
    "One of the distinctive features of a neuron is that it has a kind of activation potential: If the electric signal reaching a neuron is strong enough, the neuron will fire, sending electrical signals further along the network. Thus  there is a kind of input-output structure to neurons that the artificial networks will be mimicking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important differences \n",
    "\n",
    "Having input, output, and bias: This should so far sound _**much**_ like _**linear regression**_.\n",
    "Have the computer choose weights on the various input parameters so as to optimize the predictions on outputs.\n",
    "\n",
    "But there are, of course, **important** differences, how the model trains on the data and how it adjusts for error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Training - batches\n",
    "\n",
    "![toomuch](img/much-data-too-much-data.jpg)\n",
    "\n",
    "Unlike other models that can take all the data in the trianing set, Neural nets generally don't accept the entire dataset all at once. Thus we often want to specify a *batch size*, which will break our data up into chunks of that size.\n",
    "\n",
    "Example: $ 1,000$ data points, _batch size_ **500**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Training - epochs\n",
    "![epock](img/2014-10-28_anthropocene.png)\n",
    "\n",
    "When all four batches of 500 observations go through the NN, one **epoch** is completed. \n",
    "\n",
    "\n",
    "Generally speaking, one epoch is NOT enough to see significant error reduction. But, because of the power of gradient descent, there are often very significant strides  made after a few tens or hundreds of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back propagation - adjusting weights\n",
    "Moreover, neural nets are dynamic in the sense that, after a certain number of data points have been passed through the model, the weights will be *updated* with an eye toward optimizing our loss function. (Thinking back to biological neurons, this is like revising their activation potentials.) Typically, this is  done  by using some version of gradient descent, but [other approaches have been attempted](https://arxiv.org/abs/1605.02026).\n",
    "\n",
    "![bprop](img/BackProp_web.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back propagation - visualized\n",
    "\n",
    "![bb](img/ff-bb.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details of Back Propagation\n",
    "One of the most popular optimizers these days is called 'Adam', which generalizes from ordinary gradient descent by having individual and dynamic learning rates. [This article](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) has a nice discussion of Adam.\n",
    "\n",
    "For the mathematical details, check out [this post](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/).\n",
    "\n",
    "Given that we are only passing *some* data points through at any one time, the question of *when* to update the weights becomes pressing. Standardly, we'd wait until we've passed all the data through before updating, but we might try updating after each batch (\"batch gradient descent\") or even after each point (\"stochastic gradient descent\"). [This post](https://datascience.stackexchange.com/questions/27421/when-are-weights-updated-in-cnn) has more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Who Cares?\n",
    "\n",
    "Remember graph theory? [Neural networks are much like computational graphs](https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9). (This is why Tensorflow is useful for constructing neural networks!)\n",
    "\n",
    "And computational graphs can be used [to approximate *any* function](http://neuralnetworksanddeeplearning.com/chap4.html).\n",
    "\n",
    "I highly encourage following that last link, by the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coding a Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Let's start by constructing a generic function to mimic\n",
    "# the firing of neurons.\n",
    "\n",
    "\n",
    "def neuron_fires(input_layer, weights, biases, activation_function):\n",
    "    return activation_function(np.dot(weights, input_layer) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Now we'll choose two points at random from a standard normal\n",
    "# distribution.\n",
    "in_act = np.random.normal(-1, 1, size=2)\n",
    "\n",
    "# To fill in the weights, we want a matrix with as many columns\n",
    "# as there are input values / neurons\n",
    "# and as many rows as there are neurons in the hidden layer.\n",
    "in_to_hid = np.random.randn(3, 2)\n",
    "\n",
    "# Let's suppose a simple set of biases where each = 1.\n",
    "hid_bias = np.ones(3)\n",
    "\n",
    "# To generate the values in the hidden layer, we'll\n",
    "# execute our function!\n",
    "hid_act = neuron_fires(in_act, in_to_hid, hid_bias,\n",
    "                       activation_function=lambda x: x**2)\n",
    "\n",
    "print('Input layer: ', in_act)\n",
    "print('Weight matrix: \\n', in_to_hid)\n",
    "print('Hidden layer: ', hid_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# We'll now do the same thing for the step between\n",
    "# the hidden layer and the output layer.\n",
    "\n",
    "hid_to_out = np.random.randn(1, 3)\n",
    "out_bias = np.array([-1])\n",
    "\n",
    "print('Hidden-Output weights:', hid_to_out)\n",
    "print('Output bias:', out_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# And now we'll execute our function one more time.\n",
    "\n",
    "out_act = neuron_fires(hid_act, hid_to_out,\n",
    "                       out_bias, activation_function=lambda x: x)\n",
    "\n",
    "out_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "At this point we'd want to compare this output (perhaps rounded to 1 if we're doing a classification problem) with our observed target value so that we can adjust the weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "Some common activation functions:\n",
    "\n",
    "**binary step**: $f(x) = 0$ if $x\\leq 0$; $f(x) = 1$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Coding binary step:\n",
    "\n",
    "X = np.linspace(-10, 10, 200)\n",
    "y_bs = list(np.zeros(100))\n",
    "y_bs.extend(list(np.ones(100)))\n",
    "\n",
    "plt.plot(X, y_bs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**ReLU**: $f(x) = 0$ if $x\\leq 0$; $f(x) = x$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Coding ReLU:\n",
    "\n",
    "y_relu = list(np.zeros(100))\n",
    "y_relu.extend(np.linspace(0, 10, 100))\n",
    "\n",
    "plt.plot(X, y_relu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Sigmoid**: $f(x) = \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Coding Sigmoid:\n",
    "\n",
    "y_sig = 1 / (1 + np.exp(-X))\n",
    "\n",
    "plt.plot(X, y_sig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**tanh**: $f(x) = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Coding tanh:\n",
    "\n",
    "y_tanh = (np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))\n",
    "\n",
    "plt.plot(X, y_tanh);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Softsign**: $f(x) = \\frac{x}{1 + |x|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Coding Softsign:\n",
    "\n",
    "y_ss = X / (1 + np.abs(X))\n",
    "\n",
    "plt.plot(X, y_ss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing a multi-class classification problem, you'll want to use _Softmax_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that ReLU (\"Rectified Linear Unit\") increases without bound as $x\\rightarrow\\infty$. The advantages and drawbacks of this are discussed on [this page on stackexchange](https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GUI!\n",
    "\n",
    "[Tinker with a neural network online](https://playground.tensorflow.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
